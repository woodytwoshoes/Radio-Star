{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from luigi import Task\n",
    "from luigi.parameter import BoolParameter, IntParameter\n",
    "from luigi.task import ExternalTask\n",
    "from luigi.target import Target\n",
    "import luigi\n",
    "from csci_utils.luigi.dask.target import CSVTarget\n",
    "from csci_utils.luigi.dask.target import ParquetTarget\n",
    "from csci_utils.luigi.task import Requirement\n",
    "from csci_utils.luigi.task import Requires\n",
    "from csci_utils.luigi.task import TargetOutput\n",
    "from luigi.contrib.s3 import S3Target\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from dask import dataframe as dd\n",
    "from sklearn.metrics.pairwise import cosine_similarity, nan_euclidean_distances\n",
    "from sklearn.preprocessing import LabelEncoder, normalize\n",
    "import dask.array as da\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_objects_general(ddf, object_cols):\n",
    "    LE = LabelEncoder()\n",
    "    for object_col in object_cols:\n",
    "        ddf[object_col] = da.from_array(\n",
    "            LE.fit_transform(ddf[object_col].astype(str)))\n",
    "    return ddf\n",
    "\n",
    "def normalize_general(ddf,columns):\n",
    "    result = ddf.copy()\n",
    "    for feature_name in columns:\n",
    "        max_value = ddf[feature_name].max()\n",
    "        min_value = ddf[feature_name].min()\n",
    "        result[feature_name] = 2*(ddf[feature_name] - min_value) / (max_value - min_value) - 1\n",
    "    return result\n",
    "\n",
    "def normalize_chex(ddf, object_cols):\n",
    "    ddf = normalize_general(ddf,object_cols)\n",
    "    ddf =  normalize_general(ddf,['Age'])\n",
    "    return ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChexpertDataframe(ExternalTask):\n",
    "\n",
    "    s3_path = 's3://radio-star-csci-e-29/unzipped/'\n",
    "\n",
    "    output = TargetOutput(\n",
    "        file_pattern=\"\",\n",
    "        ext=\"train.csv\",\n",
    "        target_class=S3Target,\n",
    "        path=s3_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\data\\\\processed'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.relpath(('C:\\\\Users\\\\wmj\\\\PycharmProjects\\\\radio-star\\\\models\\\\Tasks\\\\', 'C:\\\\Users\\\\wmj\\\\PycharmProjects\\\\radio-star\\\\data\\\\processed\\\\')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessChexpertDfToParquet(Task):\n",
    "    requires = Requires()\n",
    "    chexpertdf = Requirement(ChexpertDataframe)\n",
    "\n",
    "    output = TargetOutput(\n",
    "        target_class=ParquetTarget,\n",
    "        path=\"../data/processed/\",\n",
    "        ext=\"\",\n",
    "        flag=False,\n",
    "        storage_options=dict(requester_pays=True),\n",
    "    )\n",
    "\n",
    "    def run(self):\n",
    "        pathCSV = self.input()[\"chexpertdf\"].path\n",
    "        ddf = dd.read_csv(pathCSV)\n",
    "        self.output().write_dask(ddf, compression=\"gzip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeDF(Task):\n",
    "    \"\"\"The Dataframe is best normalized before similarity calculations are\n",
    "    run on it.\"\"\"\n",
    "\n",
    "\n",
    "    requires = Requires()\n",
    "    proc_chexpertdf = Requirement(ProcessChexpertDfToParquet)\n",
    "\n",
    "    output = TargetOutput(\n",
    "        target_class=ParquetTarget,\n",
    "        path=\"../data/processed/\",\n",
    "        ext=\"\",\n",
    "        flag=False,\n",
    "        storage_options=dict(requester_pays=True),\n",
    "    )\n",
    "\n",
    "    def run(self):\n",
    "        ddf = self.input()[\"proc_chexpertdf\"].read_dask()\n",
    "        ddf_raw = ddf.copy()\n",
    "        ddf = ddf.drop(columns=['Path'])\n",
    "        object_cols = ddf.dtypes[(ddf.dtypes == object)].index.values\n",
    "\n",
    "        ddf = encode_objects_general(ddf, object_cols)\n",
    "\n",
    "        ddf = normalize_chex(ddf, object_cols)\n",
    "\n",
    "        self.output().write_dask(ddf, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FindSimilar(Task):\n",
    "    \"\"\"The Dataframe is best normalized before similarity calculations are\n",
    "    run on it.\"\"\"\n",
    "\n",
    "    requires = Requires()\n",
    "    proc_chexpertdf = Requirement(ProcessChexpertDfToParquet)\n",
    "    normalize_df = Requirement(NormalizeDF)\n",
    "    comparator_index = IntParameter(default = 37959)\n",
    "    n_images = IntParameter(default = 5)\n",
    "\n",
    "    output = TargetOutput(\n",
    "        target_class=ParquetTarget,\n",
    "        path=\"../data/processed/\",\n",
    "        ext=\"\",\n",
    "        flag=False,\n",
    "        storage_options=dict(requester_pays=True),\n",
    "    )\n",
    "\n",
    "    def run(self):\n",
    "        ddf = self.input()[\"normalize_df\"].read_dask()\n",
    "        ddf_raw = self.input()[\"proc_chexpertdf\"].read_dask()\n",
    "        \n",
    "        object_cols = ddf.dtypes[(ddf.dtypes == object)].index.values\n",
    "\n",
    "        row_comparator_raw = ddf.loc[self.comparator_index]\n",
    "\n",
    "        # This compensate for a bug in dask row equality calculations\n",
    "        row_comparator_na = row_comparator_raw.isna().compute().iloc[0]\n",
    "\n",
    "        similar_features_idx = (ddf.isna() == row_comparator_na).sum(\n",
    "            1).compute().nlargest(n=100).index\n",
    "\n",
    "        argsorted = nan_euclidean_distances(\n",
    "            row_comparator_raw.compute().values.reshape(1, -1),\n",
    "            ddf.loc[similar_features_idx.to_list()].compute().values).argsort()\n",
    "        \n",
    "        top_n = similar_features_idx[argsorted][0][:self.n_images]\n",
    "        \n",
    "        top_n_close_images = ddf_raw.loc[top_n]\n",
    "\n",
    "        self.output().write_dask(top_n_close_images, compression=\"gzip\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChexpertDataImages(ExternalTask):\n",
    "\n",
    "    s3_path = 's3://radio-star-csci-e-29/unzipped/'\n",
    "\n",
    "    output = TargetOutput(\n",
    "        file_pattern=\"\",\n",
    "        ext=\"\",\n",
    "        target_class=S3Target,\n",
    "        path=s3_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:self.target_kwargs['path'] is ../data/processed/FindSimilar\n",
      "INFO:root:BaseDaskTarget path is ../data/processed/FindSimilar/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking existence of Yelp directory\n",
      "<fsspec.implementations.local.LocalFileSystem object at 0x000002593E76B358>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FindSimilar().complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PullSimilarImages(Task):\n",
    "    \"\"\"The Dataframe is best normalized before similarity calculations are\n",
    "    run on it.\"\"\"\n",
    "\n",
    "    requires = Requires()\n",
    "    find_similar = Requirement(FindSimilar)\n",
    "    chexpert_data_images = Requirement(ChexpertDataImages)\n",
    "    \n",
    "    output = TargetOutput(\n",
    "        target_class=Target,\n",
    "        path=\"../data/processed/\",\n",
    "        ext=\"\")\n",
    "    \n",
    "    def run(self):\n",
    "        simil_dir_path = self.input()['find_similar'].path\n",
    "        simil_path = glob.glob(os.path.join(simil_dir_path,'*.parquet'))[0]\n",
    "        df = pd.read_parquet(simil_path)\n",
    "        s3_parent_dir = self.input()['chexpert_data_images'].path\n",
    "        for index, row in df_simil.iterrows():\n",
    "            rel_path = pathlib.Path(*pathlib.Path(row['Path']).parts[2:])\n",
    "            s3_img_path = os.path.join(s3_parent_dir, rel_path)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simil = pd.read_parquet(\"../data/processed/FindSimilar/part.0.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pathlib.Path(df_simil.Path.sample().values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('patient03872/study2/view1_frontal.jpg')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathlib.Path(*p.parts[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:self.target_kwargs['path'] is s3://radio-star-csci-e-29/unzipped/\n"
     ]
    }
   ],
   "source": [
    "s3_parent_dir = ChexpertDataImages().output().path\n",
    "for index, row in df_simil.iterrows():\n",
    "    rel_path = pathlib.Path(*pathlib.Path(row['Path']).parts[2:])\n",
    "    s3_img_path = os.path.join(s3_parent_dir, rel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://radio-star-csci-e-29/unzipped/patient59195\\\\study1\\\\view1_frontal.jpg'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:self.target_kwargs['path'] is ../data/processed/FindSimilar\n",
      "INFO:root:BaseDaskTarget path is ../data/processed/FindSimilar/\n"
     ]
    }
   ],
   "source": [
    "dir_path = FindSimilar().output().path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/processed/FindSimilar/*.parquet'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(dir_path,'*.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PIL'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-35f0c53dee35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mio\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'PIL'"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "class S3ImagesInvalidExtension(Exception):\n",
    "    pass\n",
    "\n",
    "class S3ImagesUploadFailed(Exception):\n",
    "    pass\n",
    "\n",
    "class S3Images(object):\n",
    "    \n",
    "    \"\"\"Useage:\n",
    "    \n",
    "        images = S3Images(aws_access_key_id='fjrn4uun-my-access-key-589gnmrn90', \n",
    "                          aws_secret_access_key='4f4nvu5tvnd-my-secret-access-key-rjfjnubu34un4tu4', \n",
    "                          region_name='eu-west-1')\n",
    "        im = images.from_s3('my-example-bucket-9933668', 'pythonlogo.png')\n",
    "        im\n",
    "        images.to_s3(im, 'my-example-bucket-9933668', 'pythonlogo2.png')\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, aws_access_key_id, aws_secret_access_key, region_name):\n",
    "        self.s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, \n",
    "                                     aws_secret_access_key=aws_secret_access_key, \n",
    "                                     region_name=region_name)\n",
    "        \n",
    "\n",
    "    def from_s3(self, bucket, key):\n",
    "        file_byte_string = self.s3.get_object(Bucket=bucket, Key=key)['Body'].read()\n",
    "        return Image.open(BytesIO(file_byte_string))\n",
    "    \n",
    "\n",
    "    def to_s3(self, img, bucket, key):\n",
    "        buffer = BytesIO()\n",
    "        img.save(buffer, self.__get_safe_ext(key))\n",
    "        buffer.seek(0)\n",
    "        sent_data = self.s3.put_object(Bucket=bucket, Key=key, Body=buffer)\n",
    "        if sent_data['ResponseMetadata']['HTTPStatusCode'] != 200:\n",
    "            raise S3ImagesUploadFailed('Failed to upload image {} to bucket {}'.format(key, bucket))\n",
    "        \n",
    "    def __get_safe_ext(self, key):\n",
    "        ext = os.path.splitext(key)[-1].strip('.').upper()\n",
    "        if ext in ['JPG', 'JPEG']:\n",
    "            return 'JPEG' \n",
    "        elif ext in ['PNG']:\n",
    "            return 'PNG' \n",
    "        else:\n",
    "            raise S3ImagesInvalidExtension('Extension is invalid') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-virtualenv-name",
   "language": "python",
   "name": "my-virtualenv-name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
